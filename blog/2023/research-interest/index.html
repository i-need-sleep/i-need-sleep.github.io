<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Research Interest | Yichen "William" Huang</title> <meta name="author" content="Yichen " william huang> <meta name="description" content="description"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://i-need-sleep.github.io/blog/2023/research-interest/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?3dd82e91913a2c1265c0f80e41ff39e2"></script> <script src="/assets/js/dark_mode.js?6458e63976eae16c0cbe86b97023895a"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <a id="top" class="ref_offset"></a> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/#top/"><span class="font-weight-bold">Yichen </span>"William" Huang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/#top">About</a> </li> <li class="nav-item"> <a class="nav-link" href="/#about_publications">Publications</a> </li> <li class="nav-item"> <a class="nav-link" href="/#about_research">Other Research</a> </li> <li class="nav-item"> <a class="nav-link" href="/#about_misc">Misc</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv.pdf"> CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research Interest</h1> <p class="post-meta">October 13, 2023</p> <p class="post-tags"> <a href="//blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Broadly interested in music AI and NLP, my research goal is to draw parallels between music and language processing and 1) devise methods for better understanding music and language models, 2) devise generalizable methods for music AI, and 3) develop effective, useful music AI applications. I have gained rich research experience under the supervision of Professors Gus Xia and Timothy Baldwin. To further grow as a researcher and contribute to these promising fields, I intend to pursue a PhD.</p> <h1 id="past-research">Past Research</h1> <h2 id="generalizable-methods-for-music-ai">Generalizable Methods for Music AI</h2> <p>Music AI encompasses a plethora of understanding and generation tasks, each with its own characteristics. I am interested in unifying them into a shared, general framework, taking inspiration from generalizable methods in NLP. In a research project supervised by Professor Gus Xia, I noted that many autoregressive music generation models are slow at inference time, prohibiting them from being used as interactive tools in music creation. However, certain generation tasks, such as harmonic style transfer, can be approximated with rule-based operations, with the transformed output refined with only minor edits. Following this intuition, I devised an edit-based model (commonly used in NLP tasks such as grammatical error correction), augmented it with a rule-based approximation step and experimented with having it learn from an autoregressive style transfer model in a distillation setup. While there exists a gap in the output’s quality, the semi-autoregressive edit model leads to a 4x improvement in inference speed, and I believe using learned rule-based transformations can make it useful for many more tasks.</p> <p>In a separate course project, I have attempted to unify a series of music information extraction (MIR) tasks under an audio-to-audio format. To accommodate this setup, I modified the transformer prior model of Jukebox with an adaptor module following ControlNet such that it can take in audio as additional condition. The advantage of this setup lies in its versatility, as many MIR tasks that involve symbolic inputs and outputs can be recast into the audio-to-audio format. For instance, MIDI-to-audio synthesis can be recast by synthesizing the input condition using simple sine waves, and music transcription can be recast by synthesizing the output. Small-scale experiments show that this method can handle tasks such as vocal source separation and MIDI-conditioned synthesis.</p> <h2 id="emergent-concepts">Emergent Concepts</h2> <p>It is a common belief that certain innate priors are behind the human understanding of concepts across different modalities. In <a href="http://www.yichenwilliamhuang.com/#sps" rel="external nofollow noopener" target="_blank">(Liu et al., 2023)</a>, published at NeurIPS 2023, we examined physical symmetry as one such high-level inductive bias. We showed that by enforcing a prior model regularized by equivariance constraints, a VAE can learn concepts such as music pitch and 3D coordinates from unlabeled perceptual data. I investigated the applicability of the approach under a more realistic setup. Specifically, I designed and conducted experiments learning from natural melodies from the Nottingham dataset and with time-invariant timber. This project helped demonstrate the commonality across models in different modalities.</p> <h1 id="future-research">Future Research</h1> <h2 id="understanding-music-models-and-language-models">Understanding Music Models and Language Models</h2> <p>In (Liu et al., 2023), we investigated a common mechanism behind different modalities, but the method cannot be easily adapted to more complex data such as polyphonic music. As it has been established that pretrained transformers are universal compute engines, I believe an alternative way to understand models and, ultimately, human concepts is to study the parallels between uni-modal models of different modalities. Music and language can serve as an ideal subject since they both involve structures and syntax. I am interested in examining the structural understanding of music and language models through methods such as probing and mechanistic interpretability methods and identifying commonalities. Findings along this line may lead to more data-efficient ways to fuse music and language models without using web-scaled data.</p> <h2 id="generalizable-methods-for-music-ai-1">Generalizable Methods for Music AI</h2> <p>I have experimented with unifying certain MIR tasks as audio-to-audio by modifying Jukebox, which is akin to sequence-to-sequence in NLP. A natural extension is to apply multi-task training. To go even further, I am interested in investigating in-context learning (ICL) and other advanced prompting methods in generative music models. Large music audio models have recently been introduced following the success of textual LLMs, but characteristic LLM features such as ICL and prompting are under-studied. I intend to start with simple setups, such as framing iterative score reduction as a chain-of-thought problem. Going further, I am interested in instructional finetuning for music audio models through adaptors.</p> <h2 id="applications-in-music-ai">Applications in Music AI</h2> <p>A strong appeal of music AI to me is that it can be directly applied to practical problems in music-making. As an amateur guitarist, a common frustration for me during recording is the laborious process of splicing and editing multiple flawed takes to produce a refined track. This complex task of recording refinement involves both performance-level (timing and techniques) and score-level (wrong notes) flaws and is under-explored in the context of music AI. An intuitive solution is to use feature fusion, where we combine the latent representations of the multiple takes and feed it into a quality-aware decoder, which may be trained with a denoising objective. However, real-world use cases may require more controllability as users may want to specify the notes or audio segments to edit. Therefore, I am also interested in applying partial re-synthesis (where we explicitly model notes, performance and timber as intermediate representations) and program synthesis (where a controller model utilises a set of pre-defined or learned edit operations). As an additional use case, such a system can be used in music tutoring by informing students of the flaws in their performance and providing demonstrations.</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container" id="footer_container"> © Copyright 2023 Yichen Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 03, 2023. </div> <script type="text/javascript">for(var randomIndexUsed=[],counter=0,numberOfPosts=1,postsTitle=["Fun fact: My Erd\u0151s number is 4 (0. Paul Erd\u0151s, 1. Nick Wormald, 2. Alistair Moffat, 3. Timothy Baldwin).","Fun fact: If you refresh this page, you may see a different fun fact.","Fun fact: Despite the time we invested/wasted on the Frankensteined Gaggia machine, I no longer drink coffee on a daily basis for health concerns. Let me know if you want to adopt the surviving victim of our retrofitting procedure.","Fun fact: To this day, I have been getting away with not knowing how exactly Fourier transform works.","Fun fact: In summer, 2020, I spent 7 days to fully memorize a lexicon of 3,000 infrequent words for GRE (including a two-day break), arriving at a sub-GPT-4 performance of 168/170 (-1) in verbal reasoning. We anticipate a larger performance gap in retention.","Fun fact: I worked as a learning assistant for an introductory machine learning course in fall, 2021 where I once had the brilliant idea of holding a last-minute review session on the night before the midterm exam. The session had a attendance exceeding the actual lectures (40+ students), and I was kindly removed from the Academic Resouce Center for acpacity concerns.","Fun fact: The first time I attempted to train a language model (of music pitches), I forgot to offset the input sequence by one step and spent two weeks wondering why the model had perfect performance.","Fun fact: If you are wondering why the guitar parts in <a href=https://open.spotify.com/track/157aKxinSWtcaNuANQwSi0?si=ec62b4ca8b7c449c> \u4f55\u4e0d\u5192\u9669 </a> sound so bad, it's because I played them (I am greatful that HEKI tried very hard to make it sound better than it is, though).","Fun fact: One of the other fun facts is generated by Copilot. Can you guess which one it is?","Fun fact: I took a class in Chinese Science Fiction literature in spring, 2021, and I am now a proud owner of a copy of <a href=https://www.amazon.com/Three-Body-Problem-Cixin-Liu/dp/0765382032> The Three-Body Problem </a> signed by the author.","How is your diffusion model going today?"];counter<numberOfPosts;){var randomIndex,postTitle;randomIndex=Math.floor(Math.random()*postsTitle.length),"-1"==randomIndexUsed.indexOf(randomIndex)&&(postTitle=postsTitle[randomIndex],document.getElementById("footer_container").innerHTML+=counter==numberOfPosts-1?"<p><center>"+postTitle+"</center></p>":"<p><center>"+postTitle+"</center></p><hr />",randomIndexUsed.push(randomIndex),counter++)}</script> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>