<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Research Interests | Yichen "William" Huang</title> <meta name="author" content="Yichen " william huang> <meta name="description" content="description"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://i-need-sleep.github.io/blog/2024/research-interests-2024dec/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?3dd82e91913a2c1265c0f80e41ff39e2"></script> <script src="/assets/js/dark_mode.js?6458e63976eae16c0cbe86b97023895a"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <a id="top" class="ref_offset"></a> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/#top/"><span class="font-weight-bold">Yichen </span>"William" Huang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/#top">About</a> </li> <li class="nav-item"> <a class="nav-link" href="/#about_publications">Publications</a> </li> <li class="nav-item"> <a class="nav-link" href="/#about_research">Other Research</a> </li> <li class="nav-item"> <a class="nav-link" href="/#about_misc">Misc</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv.pdf"> CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research Interests</h1> <p class="post-meta">March 21, 2024</p> <p class="post-tags"> <a href="//blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Recent years have seen rapid advances in music generative models. They are capable of generating high-fidelity outputs but, at the same time, are unusable as practical tools. I aim to improve music generative models into helpful, interactive tools by 1) enabling generalizable forms of controls, 2) better understanding and leveraging the human creative process behind music production, and 3) devising reliable and holistic evaluation methods. I gained rich research experience under the supervision of Professors Gus Xia, Timothy Baldwin, and Chris Donahue. To further grow as a researcher and contribute to this promising field, I intend to pursue a PhD.</p> <h1 id="past-and-ongoing-research">Past and Ongoing Research</h1> <h2 id="taming-music-generative-models">Taming Music Generative Models</h2> <p>I have been working on generalizable methods to enable such models to take on more tasks. In a project supervised by Professor Gus Xia, I attempted to enable Jukebox, an optionally lyrics-conditioned model, to handle various forms of conditions. Models like Jukebox can generate reasonably realistic music, but in practical use, users may want to apply different conditions (e.g., MIDI, chord, or melody controls). These conditions take different formats, and developing specific methods for each type of control can be impractical. I proposed audio-to-audio generation as a general setup of conditional generation. The advantage of this setup lies in its versatility, as many forms of control signals can be recast into the audio-to-audio format. For instance, MIDI-to-audio synthesis can be recast by synthesizing the input condition using simple sine waves. To accommodate this setup, I modified the transformer prior model of Jukebox with an adaptor module following ControlNet such that it can take in audio as an additional condition. Small-scale experiments show that this method can handle tasks such as MIDI-conditioned synthesis and accompaniment generation. Under the supervision of Professor Chris Donahue, I later extended this line of ideas to music information retrieval (MIR) tasks by recasting temporally varying MIR labels as audio supervision signals (e.g., beat detection can be formulated as predicting a sequence of audio clicks). I explored approaches (e.g., weight-tying, attention biases, and input patterns) to enforce locality correspondence between the input and output sequence and benchmarked our method against other methods based on generative models. While the audio-to-audio approach somewhat falls behind finetuned baselines, I believe audio-to-audio control is a unifying approach that will allow generative models a deeper understanding of music and enable more versatile applications.</p> <h2 id="automatic-evaluation-for-music-generation">Automatic Evaluation for Music Generation</h2> <p>Effective evaluation is crucial for understanding and informing the development of systems and methods, and automatic evaluation has been commonly used as a practical proxy for human judgements. During my master’s studies, I developed robust automatic metrics for evaluating machine translation and text simplification systems. By contrast, evaluation appears underexplored in music AI, where practitioners have only recently started to move on from using the dated VGGish model as evaluation backbones. This is particularly concerning considering the rapid advancement in music foundation models and applications. In an ongoing project supervised by Professors Chris Donahue and John Thickstun, we proposed a new metric for open-ended music generation based on generative models and divergence frontiers and synthesized sets of meta-evaluation data targeting fidelity, musicality, context, and diversity, where our metric overall outperforms the widely used Fréchet Audio Distance. Currently, we are conducting a listening study to verify our results and inform future research.</p> <h1 id="future-research">Future Research</h1> <h2 id="better-evaluation-for-music-generation">Better Evaluation for Music Generation</h2> <p>Evaluation is underexplored for music generation, and an immediate effort to alleviate this is to set up protocols to scalably collect ratings on music generated through one-round interactions (e.g., arena-style evaluation). In the long run, I am interested in evaluating music generation more holistically and under setups that may realistically occur during interactions. Ideally, human users interact with models in a multi-round, collaborative fashion (e.g., The user writes a melody, the model generates the accompaniment, and the user edits the generated music and writes a new melody as a continuation, repeating the process). This process involves many aspects that cannot be easily assessed by examining the end product (e.g., can the model infer the key I play in? Can it anticipate when I want to move on to the next section of the song?). This necessitates something akin to a musical Turing Test. One way to cheaply approximate this setup would be to study LM-LM collaboration and use grounded downstream metrics for fine-grained evaluation.</p> <h2 id="understanding-and-leveraging-the-creative-process">Understanding and Leveraging the Creative Process</h2> <p>Unlike language models, humans do not use a sequential, autoregressive process to create music. By using the autoregressive formulation, we are missing out on an opportunity to make models more compatible with interactive applications. I am interested in collecting and leveraging data regarding the creative process. For instance, we can collect traces of digital audio workstation (DAW) uses from processing tutorial videos or song-writing hackathons. This sort of data can be tremendously useful. Aside from building models that factorize music creation in a more human-like way, I am interested in the locality aspect of music listening (during the creative process). The free lunch of the music domain is that we do not need an eye tracker to know which part of the song the user is listening to. This provides a scalable way to collect human “attention maps”, which can then be used for distantly supervised applications (e.g., concept discovery, dynamic inference time compute allocation, etc.).</p> <h2 id="universal-control-interfaces">Universal Control Interfaces</h2> <p>Collaborative music creation with generative models involves diverse forms of control, many of which are difficult to account for beforehand (e.g., novel physical interfaces). I am interested in formulating and enabling a general way of conditioning generative models. Concretely, suppose we have a distribution of temporally varying control signals (e.g., sequences of keyboard presses corresponding to unaligned real music), can we learn a control mapping such that an acoustic music generative model’s decisions can be best explained by the key presses? One way to implement this would be to quantize the intermediate states of the acoustic model and decouple them such that one branch can maximize the sequence-level probability given by the control distribution.</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container" id="footer_container"> © Copyright 2024 Yichen Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 15, 2024. </div> <script type="text/javascript">for(var randomIndexUsed=[],counter=0,numberOfPosts=1,postsTitle=["Fun fact: My Erd\u0151s number is 4 (0. Paul Erd\u0151s, 1. Nick Wormald, 2. Alistair Moffat, 3. Timothy Baldwin).","Fun fact: If you refresh this page, you may see a different fun fact.","Fun fact: Despite the time we invested/wasted on the Frankensteined Gaggia machine, I no longer drink coffee on a daily basis for health concerns. Let me know if you want to adopt the surviving victim of our retrofitting procedure.","Fun fact: To this day, I have been getting away with not knowing how exactly Fourier transform works.","Fun fact: In summer, 2020, I spent 7 days to fully memorize a lexicon of 3,000 infrequent words for GRE (including a two-day break), arriving at a sub-GPT-4 performance of 168/170 (-1) in verbal reasoning. We anticipate a larger performance gap in retention.","Fun fact: I worked as a learning assistant for an introductory machine learning course in fall, 2021 where I once had the brilliant idea of holding a last-minute review session on the night before the midterm exam. The session had a attendance exceeding the actual lectures (40+ students), and I was kindly removed from the Academic Resouce Center for acpacity concerns.","Fun fact: The first time I attempted to train a language model (of music pitches), I forgot to offset the input sequence by one step and spent two weeks wondering why the model had perfect performance.","Fun fact: If you are wondering why the guitar parts in <a href=https://open.spotify.com/track/157aKxinSWtcaNuANQwSi0?si=ec62b4ca8b7c449c> \u4f55\u4e0d\u5192\u9669 </a> sound so bad, it's because I played them (I am greatful that HEKI tried very hard to make it sound better than it is, though).","Fun fact: One of the other fun facts is generated by Copilot. Can you guess which one it is?","Fun fact: I took a class in Chinese Science Fiction literature in spring, 2021, and I am now a proud owner of a copy of <a href=https://www.amazon.com/Three-Body-Problem-Cixin-Liu/dp/0765382032> The Three-Body Problem </a> signed by the author.","How is your diffusion model going today?",'"sudo mount /dev/sdc /mnt" -<i>Linux Horror Stories 2024 Vol. 11</i>'];counter<numberOfPosts;){var randomIndex,postTitle;randomIndex=Math.floor(Math.random()*postsTitle.length),"-1"==randomIndexUsed.indexOf(randomIndex)&&(postTitle=postsTitle[randomIndex],document.getElementById("footer_container").innerHTML+=counter==numberOfPosts-1?"<p><center>"+postTitle+"</center></p>":"<p><center>"+postTitle+"</center></p><hr />",randomIndexUsed.push(randomIndex),counter++)}</script> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>