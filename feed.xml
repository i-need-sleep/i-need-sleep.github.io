<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://i-need-sleep.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://i-need-sleep.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-15T01:18:18+00:00</updated><id>https://i-need-sleep.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Research Interest &amp;amp; Works in Progress</title><link href="https://i-need-sleep.github.io/blog/2024/research-interest-and-wip/" rel="alternate" type="text/html" title="Research Interest &amp;amp; Works in Progress"/><published>2024-03-21T10:51:21+00:00</published><updated>2024-03-21T10:51:21+00:00</updated><id>https://i-need-sleep.github.io/blog/2024/research-interest-and-wip</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2024/research-interest-and-wip/"><![CDATA[<p>My research interest is to find better ways to use and understand acoustic and symbolic music LMs. In the long run, this includes in-context learning, interaction via prompting, etc. Currently, I am working on smaller-scale projects also involving making use of music LMs.</p> <p><strong>Music LMs as regularization/search constraints</strong>. Consider the case of unsupervised music transcription with only audio data and a pretrained, frozen symbolic music LM. We apply a VQ-VAE to encode the audio data and design a way to regularize the quantized codes such that they maximize the sequence-level probability given by the symbolic LM. Going further, this can have implications for building interactive systems: Consider optimizing an instrument/synthesizer/audio edit model such that the audio output is most likely. In general, can we use pretrained language models to convey musicality/world models to upstream/downstream modules?</p> <p><strong>Multi-agent collaborative evaluation for music LMs</strong>. Human users interact with acoustic/symbolic music LMs in a multi-round, collaborative fashion (e.g. The user writes a melody, the model generates the accompaniment, and the user edits the generated music and writes a new melody as a continuation, repeating the process). We study LM-LM collaboration as a proxy of the Human-LM setup and use grounded downstream metrics (e.g. beat-tracking) for fine-grained evaluation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[description]]></summary></entry><entry><title type="html">Research Interests</title><link href="https://i-need-sleep.github.io/blog/2024/research-interests-2024dec/" rel="alternate" type="text/html" title="Research Interests"/><published>2024-03-21T10:51:21+00:00</published><updated>2024-03-21T10:51:21+00:00</updated><id>https://i-need-sleep.github.io/blog/2024/research-interests-2024dec</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2024/research-interests-2024dec/"><![CDATA[<p>Recent years have seen rapid advances in music generative models. They are capable of generating high-fidelity outputs but, at the same time, are unusable as practical tools. I aim to improve music generative models into helpful, interactive tools by 1) enabling generalizable forms of controls, 2) better understanding and leveraging the human creative process behind music production, and 3) devising reliable and holistic evaluation methods. I gained rich research experience under the supervision of Professors Gus Xia, Timothy Baldwin, and Chris Donahue. To further grow as a researcher and contribute to this promising field, I intend to pursue a PhD.</p> <h1 id="past-and-ongoing-research">Past and Ongoing Research</h1> <h2 id="taming-music-generative-models">Taming Music Generative Models</h2> <p>I have been working on generalizable methods to enable such models to take on more tasks. In a project supervised by Professor Gus Xia, I attempted to enable Jukebox, an optionally lyrics-conditioned model, to handle various forms of conditions. Models like Jukebox can generate reasonably realistic music, but in practical use, users may want to apply different conditions (e.g., MIDI, chord, or melody controls). These conditions take different formats, and developing specific methods for each type of control can be impractical. I proposed audio-to-audio generation as a general setup of conditional generation. The advantage of this setup lies in its versatility, as many forms of control signals can be recast into the audio-to-audio format. For instance, MIDI-to-audio synthesis can be recast by synthesizing the input condition using simple sine waves. To accommodate this setup, I modified the transformer prior model of Jukebox with an adaptor module following ControlNet such that it can take in audio as an additional condition. Small-scale experiments show that this method can handle tasks such as MIDI-conditioned synthesis and accompaniment generation. Under the supervision of Professor Chris Donahue, I later extended this line of ideas to music information retrieval (MIR) tasks by recasting temporally varying MIR labels as audio supervision signals (e.g., beat detection can be formulated as predicting a sequence of audio clicks). I explored approaches (e.g., weight-tying, attention biases, and input patterns) to enforce locality correspondence between the input and output sequence and benchmarked our method against other methods based on generative models. While the audio-to-audio approach somewhat falls behind finetuned baselines, I believe audio-to-audio control is a unifying approach that will allow generative models a deeper understanding of music and enable more versatile applications.</p> <h2 id="automatic-evaluation-for-music-generation">Automatic Evaluation for Music Generation</h2> <p>Effective evaluation is crucial for understanding and informing the development of systems and methods, and automatic evaluation has been commonly used as a practical proxy for human judgements. During my master’s studies, I developed robust automatic metrics for evaluating machine translation and text simplification systems. By contrast, evaluation appears underexplored in music AI, where practitioners have only recently started to move on from using the dated VGGish model as evaluation backbones. This is particularly concerning considering the rapid advancement in music foundation models and applications. In an ongoing project supervised by Professors Chris Donahue and John Thickstun, we proposed a new metric for open-ended music generation based on generative models and divergence frontiers and synthesized sets of meta-evaluation data targeting fidelity, musicality, context, and diversity, where our metric overall outperforms the widely used Fréchet Audio Distance. Currently, we are conducting a listening study to verify our results and inform future research.</p> <h1 id="future-research">Future Research</h1> <h2 id="better-evaluation-for-music-generation">Better Evaluation for Music Generation</h2> <p>Evaluation is underexplored for music generation, and an immediate effort to alleviate this is to set up protocols to scalably collect ratings on music generated through one-round interactions (e.g., arena-style evaluation). In the long run, I am interested in evaluating music generation more holistically and under setups that may realistically occur during interactions. Ideally, human users interact with models in a multi-round, collaborative fashion (e.g., The user writes a melody, the model generates the accompaniment, and the user edits the generated music and writes a new melody as a continuation, repeating the process). This process involves many aspects that cannot be easily assessed by examining the end product (e.g., can the model infer the key I play in? Can it anticipate when I want to move on to the next section of the song?). This necessitates something akin to a musical Turing Test. One way to cheaply approximate this setup would be to study LM-LM collaboration and use grounded downstream metrics for fine-grained evaluation.</p> <h2 id="understanding-and-leveraging-the-creative-process">Understanding and Leveraging the Creative Process</h2> <p>Unlike language models, humans do not use a sequential, autoregressive process to create music. By using the autoregressive formulation, we are missing out on an opportunity to make models more compatible with interactive applications. I am interested in collecting and leveraging data regarding the creative process. For instance, we can collect traces of digital audio workstation (DAW) uses from processing tutorial videos or song-writing hackathons. This sort of data can be tremendously useful. Aside from building models that factorize music creation in a more human-like way, I am interested in the locality aspect of music listening (during the creative process). The free lunch of the music domain is that we do not need an eye tracker to know which part of the song the user is listening to. This provides a scalable way to collect human “attention maps”, which can then be used for distantly supervised applications (e.g., concept discovery, dynamic inference time compute allocation, etc.).</p> <h2 id="universal-control-interfaces">Universal Control Interfaces</h2> <p>Collaborative music creation with generative models involves diverse forms of control, many of which are difficult to account for beforehand (e.g., novel physical interfaces). I am interested in formulating and enabling a general way of conditioning generative models. Concretely, suppose we have a distribution of temporally varying control signals (e.g., sequences of keyboard presses corresponding to unaligned real music), can we learn a control mapping such that an acoustic music generative model’s decisions can be best explained by the key presses? One way to implement this would be to quantize the intermediate states of the acoustic model and decouple them such that one branch can maximize the sequence-level probability given by the control distribution.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[description]]></summary></entry><entry><title type="html">Research Interest (Music AI)</title><link href="https://i-need-sleep.github.io/blog/2023/research-interest-music/" rel="alternate" type="text/html" title="Research Interest (Music AI)"/><published>2023-10-13T10:51:21+00:00</published><updated>2023-10-13T10:51:21+00:00</updated><id>https://i-need-sleep.github.io/blog/2023/research-interest-music</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2023/research-interest-music/"><![CDATA[<p>Broadly interested in Music AI and NLP, my research goal is to draw parallels between music and language processing and 1) devise methods for better understanding language and music models, 2) devise generalizable methods for music AI drawing inspirations from NLP, and 3) develop effective, useful music AI applications. I have gained rich research experience under the supervision of Professors Gus Xia and Timothy Baldwin. To further grow as a researcher and contribute to these promising fields, I intend to pursue a PhD.</p> <h1 id="past-research">Past Research</h1> <h2 id="generalizable-methods-for-music-ai">Generalizable Methods for Music AI</h2> <p>Music AI encompasses a plethora of understanding and generation tasks, each with its own characteristics. I am interested in unifying them into a shared, general framework, taking inspiration from generalizable methods in NLP. In a research project supervised by Professor Gus Xia, I have attempted to unify a series of music information retrieval (MIR) tasks under an audio-to-audio format. To accommodate this setup, I modified the transformer prior model of Jukebox with an adaptor module following ControlNet such that it can take in additional audio as condition. The advantage of this setup lies in its versatility, as many MIR tasks that involve symbolic inputs and outputs can be recast into the audio-to-audio format. For instance, MIDI-to-audio synthesis can be recast by synthesizing the input condition using simple sine waves, and music transcription can be recast by synthesizing the output. Small-scale experiments show that this method can handle tasks such as vocal source separation and MIDI-conditioned synthesis.</p> <p>Another common challenge in music AI and NLP is to improve the inference time of autoregressive generation models. This is particularly an impediment in utilizing music AI models as interactive tools. In another project, I noted that certain generation tasks, such as harmonic style transfer, can be approximated with rule-based operations, with the transformed output refined with only minor edits. Following this intuition, I devised an edit-based model (commonly used in NLP tasks such as grammatical error correction) that accommodates symbolic music, augmented it with a rule-based approximation step and experimented with having it learn from an autoregressive style transfer model in a distillation setup. While there exists a gap in the output’s quality, the semi-autoregressive edit model leads to a 4x improvement in inference speed, and I believe using learned rule-based transformations can make it useful for many more tasks.</p> <h2 id="emergent-concepts">Emergent Concepts</h2> <p>It is a common belief that certain innate priors enable the human understanding of concepts across different modalities. Previous work has relied on domain knowledge to induce human-aligned concept representations with self-supervised learning. In <a href="http://www.yichenwilliamhuang.com/#sps">(Liu et al., 2023)</a>, published at NeurIPS 2023, we examined physical symmetry as a minimal unifying inductive bias applicable to different concepts across modalities. We assume that sets of symmetric relationships (e.g. translational equivariance) govern the dynamics of human concepts (sequences of pitches). Following this assumption, we implement a VAE with an equivariance-regularized prior model and show that it can discover concepts such as music pitch and 3D coordinates from unlabeled perceptual data with minimal domain knowledge. Within this project, I designed and conducted experiments learning from natural melodies and with time-invariant timber to apply our method to more real-world setups. This project helped demonstrate the commonality across models in different modalities.</p> <h1 id="future-research">Future Research</h1> <h2 id="understanding-music-models-and-language-models">Understanding Music Models and Language Models</h2> <p>Previous work on NLP interpretability has used probing to align the intermediate representations with human concepts, which rely on and are limited by pre-defined linguistic knowledge. This limitation is more salient when it comes to music, where the domain knowledge is more ambiguous. This motivates a search for a different anchor, or proxy, of interpretable concepts. In (Liu et al., 2023), we investigated physical symmetry as one such anchor, but the method cannot be easily adapted to more complex data such as natural language and polyphonic music. Since it has been established that pretrained transformers are universal compute engines, I believe an alternative way to understand models and, ultimately, human concepts is to study the parallels between uni-modal models of different modalities. Music and language can serve as an ideal subject since they both involve structure and syntax. I am interested in examining the structural understanding of music and language models through methods such as probing and mechanistic approaches and identifying commonalities. In the long run, I envision that we can identify and manipulate the implicit functions behind language and music processing such that we can fuse language and music models in a much more data-efficient manner.</p> <h2 id="generalizable-methods-for-music-ai-1">Generalizable Methods for Music AI</h2> <p>Past work in music AI has predominantly focused on utilizing domain knowledge for solving individual, specific tasks. Recent breakthroughs in generalizable methods in NLP (e.g. LLM prompting and parameter-efficient finetuning) offer a new opportunity to unify previously disparate music tasks. I have experimented with unifying certain MIR tasks as audio-to-audio by modifying Jukebox, which is akin to sequence-to-sequence in NLP. Following the historical trajectory of NLP paradigms (seq2seq to T5-like multi-tasking to LLM-based prompting), a natural extension is to apply multi-task training. To go even further, I am interested in investigating in-context learning (ICL) and other advanced prompting methods in generative music models. Large music audio models have recently been introduced following the success of textual LLMs, but characteristic LLM features such as ICL and prompting are understudied. Enabling such features would improve controllability and expressiveness and unify complex music tasks such as motif-conditioned generation or music genre transfer under a single, powerful model. I intend to start with simple setups, such as framing iterative score reduction as a chain-of-thought problem. Going further, a key impediment is the ambiguity of describing music with music (e.g. <a href="https://openreview.net/forum?id=sn7CYWyavh&amp;noteId=3X6BSBDIPB">(Anonymous, 2023)</a> examined reduced music scores as an intermediate product in generation, but tasks like music summarization are difficult to define). I am interested in instructional finetuning for music audio models through adaptors.</p> <h2 id="applications-in-music-ai">Applications in Music AI</h2> <p>A strong appeal of music AI to me is that it can be directly applied to practical problems in music-making. As an amateur guitarist, a common frustration for me during recording is the laborious process of splicing and editing multiple flawed takes to produce a refined track. This complex task of recording refinement involves both performance-level (timing and techniques) and score-level (wrong notes) flaws and is under-explored in the context of neural methods. An intuitive solution is to use feature fusion, where we combine the latent representations of the multiple takes and feed them into a quality-aware decoder trained with a denoising objective. However, real-world use cases require more controllability as users may want to specify particular notes/audio segments or specific aspects (timing, velocity, etc.) to edit. Therefore, a superior approach can be to apply partial re-synthesis (where we explicitly model notes, performance, and timber as intermediate representations) or program synthesis (where a controller model utilizes a set of pre-defined or learned edit operations). As an additional use case, such a system can be used in music tutoring to inform students of the flaws in their performance and provide demonstrations.</p> ]]></content><author><name></name></author><summary type="html"><![CDATA[description]]></summary></entry><entry><title type="html">Post Template</title><link href="https://i-need-sleep.github.io/blog/2023/template/" rel="alternate" type="text/html" title="Post Template"/><published>2023-10-13T10:51:21+00:00</published><updated>2023-10-13T10:51:21+00:00</updated><id>https://i-need-sleep.github.io/blog/2023/template</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2023/template/"><![CDATA[<p>Under construction LOL</p> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h4 id="hipster-list">Hipster list</h4> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> <p>Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90’s yr typewriter selfies letterpress cardigan vegan.</p> <hr/> <p>Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.</p> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <p>Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[description]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://i-need-sleep.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://i-need-sleep.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>