<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://i-need-sleep.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://i-need-sleep.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-30T20:39:08+00:00</updated><id>https://i-need-sleep.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Research Interest &amp;amp; Works in Progress</title><link href="https://i-need-sleep.github.io/blog/2024/research-interest-and-wip/" rel="alternate" type="text/html" title="Research Interest &amp;amp; Works in Progress"/><published>2024-03-21T10:51:21+00:00</published><updated>2024-03-21T10:51:21+00:00</updated><id>https://i-need-sleep.github.io/blog/2024/research-interest-and-wip</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2024/research-interest-and-wip/"><![CDATA[<p>My research interest is to find better ways to use and understand acoustic and symbolic music LMs. In the long run, this includes in-context learning, interaction via prompting, etc. Currently, I am working on smaller-scale projects also involving making use of music LMs.</p> <p><strong>Music LMs as regularization/search constraints</strong>. Consider the case of unsupervised music transcription with only audio data and a pretrained, frozen symbolic music LM. We apply a VQ-VAE to encode the audio data and design a way to regularize the quantized codes such that they maximize the sequence-level probability given by the symbolic LM. Going further, this can have implications for building interactive systems: Consider optimizing an instrument/synthesizer/audio edit model such that the audio output is most likely. In general, can we use pretrained language models to convey musicality/world models to upstream/downstream modules?</p> <p><strong>Multi-agent collaborative evaluation for music LMs</strong>. Human users interact with acoustic/symbolic music LMs in a multi-round, collaborative fashion (e.g. The user writes a melody, the model generates the accompaniment, and the user edits the generated music and writes a new melody as a continuation, repeating the process). We study LM-LM collaboration as a proxy of the Human-LM setup and use grounded downstream metrics (e.g. beat-tracking) for fine-grained evaluation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[description]]></summary></entry><entry><title type="html">Research Interest (Music AI)</title><link href="https://i-need-sleep.github.io/blog/2023/research-interest-music/" rel="alternate" type="text/html" title="Research Interest (Music AI)"/><published>2023-10-13T10:51:21+00:00</published><updated>2023-10-13T10:51:21+00:00</updated><id>https://i-need-sleep.github.io/blog/2023/research-interest-music</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2023/research-interest-music/"><![CDATA[<p>Broadly interested in Music AI and NLP, my research goal is to draw parallels between music and language processing and 1) devise methods for better understanding language and music models, 2) devise generalizable methods for music AI drawing inspirations from NLP, and 3) develop effective, useful music AI applications. I have gained rich research experience under the supervision of Professors Gus Xia and Timothy Baldwin. To further grow as a researcher and contribute to these promising fields, I intend to pursue a PhD.</p> <h1 id="past-research">Past Research</h1> <h2 id="generalizable-methods-for-music-ai">Generalizable Methods for Music AI</h2> <p>Music AI encompasses a plethora of understanding and generation tasks, each with its own characteristics. I am interested in unifying them into a shared, general framework, taking inspiration from generalizable methods in NLP. In a research project supervised by Professor Gus Xia, I have attempted to unify a series of music information retrieval (MIR) tasks under an audio-to-audio format. To accommodate this setup, I modified the transformer prior model of Jukebox with an adaptor module following ControlNet such that it can take in additional audio as condition. The advantage of this setup lies in its versatility, as many MIR tasks that involve symbolic inputs and outputs can be recast into the audio-to-audio format. For instance, MIDI-to-audio synthesis can be recast by synthesizing the input condition using simple sine waves, and music transcription can be recast by synthesizing the output. Small-scale experiments show that this method can handle tasks such as vocal source separation and MIDI-conditioned synthesis.</p> <p>Another common challenge in music AI and NLP is to improve the inference time of autoregressive generation models. This is particularly an impediment in utilizing music AI models as interactive tools. In another project, I noted that certain generation tasks, such as harmonic style transfer, can be approximated with rule-based operations, with the transformed output refined with only minor edits. Following this intuition, I devised an edit-based model (commonly used in NLP tasks such as grammatical error correction) that accommodates symbolic music, augmented it with a rule-based approximation step and experimented with having it learn from an autoregressive style transfer model in a distillation setup. While there exists a gap in the outputâ€™s quality, the semi-autoregressive edit model leads to a 4x improvement in inference speed, and I believe using learned rule-based transformations can make it useful for many more tasks.</p> <h2 id="emergent-concepts">Emergent Concepts</h2> <p>It is a common belief that certain innate priors enable the human understanding of concepts across different modalities. Previous work has relied on domain knowledge to induce human-aligned concept representations with self-supervised learning. In <a href="http://www.yichenwilliamhuang.com/#sps">(Liu et al., 2023)</a>, published at NeurIPS 2023, we examined physical symmetry as a minimal unifying inductive bias applicable to different concepts across modalities. We assume that sets of symmetric relationships (e.g. translational equivariance) govern the dynamics of human concepts (sequences of pitches). Following this assumption, we implement a VAE with an equivariance-regularized prior model and show that it can discover concepts such as music pitch and 3D coordinates from unlabeled perceptual data with minimal domain knowledge. Within this project, I designed and conducted experiments learning from natural melodies and with time-invariant timber to apply our method to more real-world setups. This project helped demonstrate the commonality across models in different modalities.</p> <h1 id="future-research">Future Research</h1> <h2 id="understanding-music-models-and-language-models">Understanding Music Models and Language Models</h2> <p>Previous work on NLP interpretability has used probing to align the intermediate representations with human concepts, which rely on and are limited by pre-defined linguistic knowledge. This limitation is more salient when it comes to music, where the domain knowledge is more ambiguous. This motivates a search for a different anchor, or proxy, of interpretable concepts. In (Liu et al., 2023), we investigated physical symmetry as one such anchor, but the method cannot be easily adapted to more complex data such as natural language and polyphonic music. Since it has been established that pretrained transformers are universal compute engines, I believe an alternative way to understand models and, ultimately, human concepts is to study the parallels between uni-modal models of different modalities. Music and language can serve as an ideal subject since they both involve structure and syntax. I am interested in examining the structural understanding of music and language models through methods such as probing and mechanistic approaches and identifying commonalities. In the long run, I envision that we can identify and manipulate the implicit functions behind language and music processing such that we can fuse language and music models in a much more data-efficient manner.</p> <h2 id="generalizable-methods-for-music-ai-1">Generalizable Methods for Music AI</h2> <p>Past work in music AI has predominantly focused on utilizing domain knowledge for solving individual, specific tasks. Recent breakthroughs in generalizable methods in NLP (e.g. LLM prompting and parameter-efficient finetuning) offer a new opportunity to unify previously disparate music tasks. I have experimented with unifying certain MIR tasks as audio-to-audio by modifying Jukebox, which is akin to sequence-to-sequence in NLP. Following the historical trajectory of NLP paradigms (seq2seq to T5-like multi-tasking to LLM-based prompting), a natural extension is to apply multi-task training. To go even further, I am interested in investigating in-context learning (ICL) and other advanced prompting methods in generative music models. Large music audio models have recently been introduced following the success of textual LLMs, but characteristic LLM features such as ICL and prompting are understudied. Enabling such features would improve controllability and expressiveness and unify complex music tasks such as motif-conditioned generation or music genre transfer under a single, powerful model. I intend to start with simple setups, such as framing iterative score reduction as a chain-of-thought problem. Going further, a key impediment is the ambiguity of describing music with music (e.g. <a href="https://openreview.net/forum?id=sn7CYWyavh&amp;noteId=3X6BSBDIPB">(Anonymous, 2023)</a> examined reduced music scores as an intermediate product in generation, but tasks like music summarization are difficult to define). I am interested in instructional finetuning for music audio models through adaptors.</p> <h2 id="applications-in-music-ai">Applications in Music AI</h2> <p>A strong appeal of music AI to me is that it can be directly applied to practical problems in music-making. As an amateur guitarist, a common frustration for me during recording is the laborious process of splicing and editing multiple flawed takes to produce a refined track. This complex task of recording refinement involves both performance-level (timing and techniques) and score-level (wrong notes) flaws and is under-explored in the context of neural methods. An intuitive solution is to use feature fusion, where we combine the latent representations of the multiple takes and feed them into a quality-aware decoder trained with a denoising objective. However, real-world use cases require more controllability as users may want to specify particular notes/audio segments or specific aspects (timing, velocity, etc.) to edit. Therefore, a superior approach can be to apply partial re-synthesis (where we explicitly model notes, performance, and timber as intermediate representations) or program synthesis (where a controller model utilizes a set of pre-defined or learned edit operations). As an additional use case, such a system can be used in music tutoring to inform students of the flaws in their performance and provide demonstrations.</p> ]]></content><author><name></name></author><summary type="html"><![CDATA[description]]></summary></entry><entry><title type="html">Post Template</title><link href="https://i-need-sleep.github.io/blog/2023/template/" rel="alternate" type="text/html" title="Post Template"/><published>2023-10-13T10:51:21+00:00</published><updated>2023-10-13T10:51:21+00:00</updated><id>https://i-need-sleep.github.io/blog/2023/template</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2023/template/"><![CDATA[<p>Under construction LOL</p> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h4 id="hipster-list">Hipster list</h4> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> <p>Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90â€™s yr typewriter selfies letterpress cardigan vegan.</p> <hr/> <p>Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.</p> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. â€”Anais Nin </blockquote> <p>Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[description]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://i-need-sleep.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://i-need-sleep.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://i-need-sleep.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>